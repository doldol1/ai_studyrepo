{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"REopuI_ItQ-0","executionInfo":{"status":"ok","timestamp":1687425818998,"user_tz":-540,"elapsed":5775,"user":{"displayName":"GD G (왕철면피)","userId":"08503996268701527706"}}},"outputs":[],"source":["import torch\n","import glob\n","import os\n","# glob 결과 숫자 오름차순으로 정리해주는 라이브러리, 기능적으로 필요하지 않았음을 깨달았으나\n","# 정렬 작업이 유지보수를 가정했을 때 충분히 의미 있다고 생각해서 그냥 놔두기로 함\n","import natsort\n","from PIL import Image\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","from torch import nn\n","from torchvision import models\n","from torchsummary import summary\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import numpy as np\n","import time"]},{"cell_type":"code","source":["# 구글 드라이브 마운트\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Bo7Vw5litWYp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")"],"metadata":{"id":"h7hvx9TS_df9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Vkt78evtQ-5"},"outputs":[],"source":["# transform을 적용한 커스텀 데이터셋\n","# 무조건 torch.utils.data.Dataset을 상속받아야 한다.\n","class cnd_data(torch.utils.data.Dataset):\n","    def __init__(self, file_path, train=True, transforms=None):\n","\n","        self.train=train\n","        self.transforms=transforms\n","\n","        # cat, dog 경로 설정\n","        # self.cat_img_path=os.path.join(file_path, 'data\\kagglecatsanddogs\\PetImages\\Cat')\n","        # self.dog_img_path=os.path.join(file_path, 'data\\kagglecatsanddogs\\PetImages\\Dog')\n","        self.cat_img_path=os.path.join(file_path, 'data/kagglecatsanddogs/PetImages/Cat')\n","        self.dog_img_path=os.path.join(file_path, 'data/kagglecatsanddogs/PetImages/Dog')\n","        print(self.dog_img_path)\n","\n","        # cat, dog 이미지 목록 불러오기\n","        self.cat_list=natsort.natsorted(glob.glob(self.cat_img_path + '/*.jpg'))\n","        self.dog_list=natsort.natsorted(glob.glob(self.dog_img_path + '/*.jpg'))\n","\n","        # cat, dog 이미지 list 및 label 지정하기, 0은 cat이고, 1은 dog이다\n","        # cat, dog 각각 12500개의 이미지가 존재하며, 각각 12000개는 train, 500개는 test에 사용된다\n","        if self.train==True:\n","            self.imgn_list=self.cat_list[:12000]+self.dog_list[:12000]\n","            self.img_label=[0]*12000+[1]*12000\n","\n","        else:\n","            self.imgn_list=self.cat_list[12000:]+self.dog_list[12000:]\n","            self.img_label=[0]*500+[1]*500\n","\n","        # 한번에 모든 이미지를 메모리에 올리고 싶었지만 공간 부족으로 불가\n","        # getitem쪽에 올렸다.\n","\n","\n","    # __len__()은 데이터쌍의 개수를 의미한다.\n","    # 아마 __len__의 크기를 기준으로 Dataloader에서 batch 묶음의 수를 결정하고\n","    # __len__만큼의 데이터쌍을 가져오는 것 같다.\n","    def __len__(self):\n","        return len(self.img_label)\n","\n","    # __getitem__()은 하나의 데이터쌍(보통 데이터, 레이블)을 가져오는데 사용된다.\n","    # __getitem__출력시 한 쌍의 데이터가 아니라 한 batch만큼을 한번에 불러오는 방식으로 짜고 싶었지만\n","    # (만약 그렇게 한다면 Dataloader에서 불러온 다음 중첩 for문을 사용하여 사용하게 될 것이다.)\n","    # 처음으로 짜는 커스텀 데이터셋이기 때문에 한 쌍의 데이터를 가져올 때마다\n","    def __getitem__(self, idx):\n","\n","        # 원 데이터는 cat과 dog 폴더로 나뉘어 있으며, 각각 0~12499까지 숫자가 파일 이름으로 사용된다.\n","        # 또한, train은 0~11999, test는 12000~12499 를 파일 이름으로 사용한다.\n","        # train 기준 실존하는 imgn_list의 index는 0~23999까지 사용하게 되므로,\n","        # 0~11999 idx의 경우 cat폴더에서 가져와야 하며,\n","        # 12000~23999 idx의 경우 dog 폴더에서 가져와야 한다.(당연히 dog폴더의 train이미지는 0~11999이므로 숫자 변환도 필요하다)\n","        # 라고 처음에는 생각해 왔지만 헛생각이었다... 어차피 인덱스와 이에 해당하는 이미지 경로는 연결되어 있으니 추가적인 조치를 취하지 않고도\n","        # 문제를 해결할 수 있다.\n","        image_data=Image.open(self.imgn_list[idx]).convert('RGB')\n","\n","\n","        # if len(np.array(image_data).shape)==2:\n","        #     image_data=image_data.convert('RGB')\n","        #     print('변환 후 사이즈:',np.array(image_data).shape)\n","\n","        if self.transforms:\n","            sample=self.transforms(image_data)\n","\n","        # print('사이즈:', sample.size())\n","        # 이미지에서 channel이 3이 아닌 경우\n","        # if sample.size()[0] != 3:\n","        #     print(self.imgn_list[idx])\n","        #     print('변환 사이즈:', sample.size())\n","\n","            # sample=sample.expand(3, -1, -1)\n","            # print(sample.size())\n","\n","        return sample, self.img_label[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mCFV3nEDtQ-8"},"outputs":[],"source":["# 경로 설정, py파일로 변환시 경로는 변경되어야 한다.\n","# local path\n","# path=os.path.abspath('../')\n","# colab path\n","path=os.path.abspath('./drive/MyDrive/Colab_Notebooks/')\n","\n","# Resize: 크기를 224, 224로 맞춘다\n","# ToTensor: 데이터 타입을 Tensor로 만든다. Tensor의 원소는 0~1로 정해진다.(https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor)\n","# custom으로 transform를 작성하는 것도 가능하다.\n","transforms=transforms.Compose([\n","    transforms.Resize(size=(224, 224)),\n","    transforms.ToTensor()])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qVJ1kF6GtQ--"},"outputs":[],"source":["cnd_train=cnd_data(file_path=path, train=True, transforms=transforms)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gmwCzZu-tQ-_"},"outputs":[],"source":["cnd_dataloader=DataLoader(cnd_train, batch_size=32, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a81U4uKFtQ_A"},"outputs":[],"source":["# resnet34나 50이나 조금씩 원문에서 주장하는 model을 수정한 듯한 흔적이 보인다.\n","# 하지만 지금은 resnet 원문의 것을 구현하는 입장이기 때문에\n","# 모델 참조를 하다 원문과 다른 부분이 있다면 무시하고 원문대로 한다.\n","# resnet34_preset=models.resnet34()\n","# summary(resnet34_preset, input_size=(3, 224, 224))\n","# print(resnet34_preset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5vTLO01etQ_D"},"outputs":[],"source":["# resnet50_preset=models.resnet50()\n","# summary(resnet50_preset, input_size=(3, 224, 224))\n","# print(resnet50_preset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6JnKJcZtQ_E"},"outputs":[],"source":["# 모델을 정의할 때는 무조건 torch.nn.Module을 상속받아야 한다.\n","# block은 short connection이 있는 최소 단위이며\n","# group은 동일한 block 형성 패턴(논문 table 1의 conv2_x, conv3_x)을 의미한다.\n","class ResNet_compat(nn.Module):\n","    def __init__(self,\n","                 input_shape=(3, 224, 224),\n","                 blocks_in_model=[3, 4, 6, 3],\n","                 layers_in_block=[2, 2, 2, 2],\n","                 kernel_sizes=[(3,3), (3,3), (3,3), (3,3)],\n","                 channel_sizes=[(64,64), (128,128), (256,256), (512,512)],\n","                 class_size=2,\n","                 is_plain=False):\n","\n","        super(ResNet_compat, self).__init__()\n","\n","        self.input_shape=input_shape\n","        self.blocks_in_model=blocks_in_model\n","        self.layers_in_block=layers_in_block\n","        self.kernel_sizes=kernel_sizes\n","        self.channel_sizes=channel_sizes\n","        self.class_size=class_size\n","        self.is_plain=is_plain\n","\n","\n","        # pytorch에도 padding='same'이라는 옵션은 존재하지만, stride=1일\n","        # 경우만 사용 가능하다.\n","        # 아래 코드는 (W-F+2P)/S + 1 공식 적용한 코드로\n","        # 계산 결과가 소수점이 나오지만, pytorch에서 사용하는 resnet이 이렇게 설정하였기 때문에\n","        # 똑같이 진행한다.\n","\n","        # conv1+conv2 maxpooling\n","        self.conv1=nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n","            nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n","            nn.ReLU(), # 왜 그런지 모르겠지만 preset에는 inplace=True(default: False)\n","            nn.MaxPool2d(kernel_size=(3,3), stride=2, padding=1)\n","        )\n","\n","        # conv2(maxpooling은 제외), short connection은 구현부에서 구현\n","\n","        self.block_forms=nn.Sequential()\n","        for i in range(len(blocks_in_model)):\n","            for j in range(blocks_in_model[i]):\n","\n","                # i는 group, j는 block\n","                if i==0 and j==0:\n","                    inp_channel=64\n","                elif i!=0 and j==0:\n","                    inp_channel=channel_sizes[i-1][-1]\n","                else:\n","                    inp_channel=False\n","\n","                # # input_channel은 block의 맨 처음+group(group: block묶음)의 맨 처음인 경우만 사용됨\n","                # self.block_forms.add_module(nn.Sequential(*self.build_block(\n","                #     self.layers_in_block[i],\n","                #     kernel_sizes=self.kernel_sizes[i],\n","                #     channel_sizes=self.channel_sizes[i],\n","                #     input_channel=inp_channel,\n","                #     is_plain=False\n","                # )))\n","                # input_channel은 block의 맨 처음+group(group: block묶음)의 맨 처음인 경우만 사용됨\n","                self.block_forms.add_module(f'bblock{i, j}',self.build_block(\n","                    self.layers_in_block[i],\n","                    kernel_sizes=self.kernel_sizes[i],\n","                    channel_sizes=self.channel_sizes[i],\n","                    input_channel=inp_channel,\n","                    is_plain=False\n","                ))\n","\n","\n","\n","        # summary가 안 먹혀서 새로 짠 코드\n","        # self.model_body=nn.Sequential(*self.block_forms)\n","\n","        # 선언을 하려고 하면 추가 입력이 필요해서 forward에 설정해야 하는 상황...\n","        self.relu = nn.ReLU()\n","\n","        self.end_avg2d=nn.AdaptiveAvgPool2d((1,1))\n","        self.end_linear=nn.Linear(in_features=self.channel_sizes[-1][-1], out_features=2, bias=True)\n","        self.end_softmax=nn.Softmax(-1)\n","\n","    # input_channel은 block의 맨 처음+group(group: block묶음)의 맨 처음인 경우만 사용됨\n","    def build_block(self, layers, kernel_sizes, channel_sizes, input_channel, is_plain=False):\n","\n","        full_block=nn.Sequential()\n","        for i in range(layers):\n","            if kernel_sizes[i]!= 1:\n","                layer_padding=(1,1)\n","            else:\n","                layer_padding=(0,0)\n","\n","            if input_channel and i==0:\n","                if input_channel != channel_sizes[i]:\n","                    f_stride=2\n","                else:\n","                    f_stride=1\n","                full_block.add_module(f'conv{i}',nn.Conv2d(in_channels=input_channel,\n","                                            out_channels=channel_sizes[i],\n","                                            kernel_size=kernel_sizes[i],\n","                                            padding=layer_padding,\n","                                            stride=f_stride,\n","                                            bias=False\n","                                            ))\n","            else:\n","                # 50이상은 channel이 일시적으로 늘어나도 feature map의 크기가 그대로임음 명심할 것\n","                # padding만 어떻게 할지 고민해보자... kernel_size가 1일 때는 패딩 제외?? 아니면 3일 때만 padding 1??\n","                full_block.add_module(f'conv{i}',nn.Conv2d(in_channels=channel_sizes[i-1],\n","                                            out_channels=channel_sizes[i],\n","                                            kernel_size=kernel_sizes[i],\n","                                            padding=layer_padding,\n","                                            bias=False\n","                                            ))\n","            # batch_normalization\n","            full_block.add_module(f'batnorm{i}',nn.BatchNorm2d(channel_sizes[i], eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n","\n","            # short connection은 구현부에서 만들기 때문에 block의 마지막 layer가 아니라면 relu 추가\n","            # inplace 옵션이 있고, pre-model에서는 사용하긴 사용하지만 왜 사용하는지 모르겠어서 사용 안함\n","            if i< layers-1:\n","                full_block.add_module(f'relu{i}',nn.ReLU())\n","\n","        return full_block\n","\n","    def forward(self, x):\n","        # print(x.shape)\n","        x=self.conv1(x)\n","        # print(x.shape)\n","\n","        # model body\n","        for block in self.block_forms:\n","            block= block # 만약 구조적 문제가 해결되면 삭제 시도해볼 것\n","            identity=x\n","            x=block(x)\n","\n","            if block[0].in_channels != block[0].out_channels:\n","                self.reduce=nn.Conv2d(\n","                    block[0].in_channels,\n","                    block[0].out_channels,\n","                    kernel_size=(1,1),\n","                    stride=2).to(device)\n","                identity=self.reduce(identity)\n","\n","            x+=identity\n","            x=self.relu(x)\n","\n","\n","        # 끝단\n","        x=self.end_avg2d(x)\n","        x=torch.flatten(x, 1, -1)\n","\n","        # x현재 shape는 [2, 512, 1, 1]\n","        x=self.end_linear(x)\n","        x=self.end_softmax(x)\n","\n","        return x\n","\n","        # return self.conv_temp(x)\n","\n","        pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uN4V5FULtQ_F"},"outputs":[],"source":["train_model=ResNet_compat().to(device)\n","summary(train_model, input_size=(3, 224, 224))\n","print(train_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bG9T-X7FtQ_G"},"outputs":[],"source":["learning_rate=0.01\n","\n","loss_f= nn.CrossEntropyLoss()\n","# train_model.parameters: 최적화할 대상의 파라미터\n","# lr=learning_rate\n","optimizer = torch.optim.SGD(train_model.parameters(), lr=learning_rate)\n","writer=SummaryWriter()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWooXKQWtQ_H"},"outputs":[],"source":["# dogs 11285, 8730, 11675 3588, 5604(not dog), 11853, 2877, 6318, 9078(channel 4), 11410 /3588와 5604가 중복해서 나옴. 특정 데이터 문제일 가능성이 높아짐\n","# cats 8470, 5686, 9778, 2877, 7276, 11935, 5370\n","EPOCHS=30\n","\n","for epoch in range(EPOCHS):\n","    losses=[]\n","    # running loss: 5batch동안 loss 누적값\n","    running_loss=0\n","    # epoch_total_loss: 1 epoch에서 발생한 loss 누적값, 750(1 epoch batch수)을 나눠서 평균 loss값을 구하는데 사용할 예정\n","    epoch_total_loss=0\n","    start_time=time.time()\n","\n","\n","    for i, inp in enumerate(cnd_dataloader):\n","\n","        inputs, labels= inp\n","        inputs, labels= inputs.to(device), torch.Tensor(labels).to(device)\n","        # 모든 gradient를 0으로 설정, 이렇게 하지 않으면 이전 loop의 gradient값이 그대로 남아있어 제대로 학습이 되지 않는다.\n","        optimizer.zero_grad()\n","\n","        # train_model을 태운 다음 loss를 계산한다.\n","        outputs= train_model(inputs)\n","        loss= loss_f(outputs, labels)\n","        # 계산한 loss를 losses에 추가한다.\n","        losses.append(loss.item())\n","\n","        # loss.backward()로 gradient를 계산하고\n","        # optimizer를 사용하여 반영한다.\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss+=loss.item()\n","\n","        if i%5 == 0 and i>0:\n","            end_time=time.time()\n","\n","            print(f'Loss [{epoch+1}, {i}](epoch, minibatch): ', running_loss/100)\n","            print('time taken:', end_time-start_time)\n","            start_time=end_time\n","            epoch_total_loss+=running_loss\n","            running_loss=0.0\n","\n","    writer.add_scalar(\"Loss / epoch \", epoch_total_loss/len(cnd_data), i)\n","    torch.save(train_model.state_dict(), f'model_weights_{epoch}.pth')\n","\n","    # avg_loss= sum(losses)/len(losses)\n","\n","writer.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4EYgxyItQ_J"},"outputs":[],"source":["# no gpu\n","# Loss [1, 5](epoch, minibatch):  0.04158005654811859\n","# time taken: 184.4762670993805\n","# Loss [1, 10](epoch, minibatch):  0.034206237196922305\n","# time taken: 139.53229141235352\n","# Loss [1, 15](epoch, minibatch):  0.03538666427135467\n","# time taken: 144.29514956474304\n","# Loss [1, 20](epoch, minibatch):  0.03478239357471466\n","# time taken: 142.34847784042358\n","# Loss [1, 25](epoch, minibatch):  0.0355976140499115\n","# time taken: 146.32465648651123\n","# Loss [1, 30](epoch, minibatch):  0.03640728116035461\n","# time taken: 143.51593947410583\n","# Loss [1, 35](epoch, minibatch):  0.034946812391281126\n","# time taken: 145.67319536209106\n","# Loss [1, 40](epoch, minibatch):  0.034260995388031006\n","# time taken: 146.46410942077637\n","# Loss [1, 45](epoch, minibatch):  0.035030335783958436\n","# time taken: 148.0001323223114\n","# Loss [1, 50](epoch, minibatch):  0.034637700319290164\n","# time taken: 140.45694208145142"]},{"cell_type":"code","source":["# gpu\n","# Loss [1, 5](epoch, minibatch):  0.04298167526721954\n","# time taken: 99.36302304267883\n","# Loss [1, 10](epoch, minibatch):  0.03436670839786529\n","# time taken: 74.46278643608093\n","# Loss [1, 15](epoch, minibatch):  0.034922510385513306\n","# time taken: 76.45175623893738\n","# Loss [1, 20](epoch, minibatch):  0.03464948296546936\n","# time taken: 74.64323735237122\n","# Loss [1, 25](epoch, minibatch):  0.03538934946060181\n","# time taken: 75.90337038040161"],"metadata":{"id":"XsmXwSCK7uaY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"H9h3h39_8ENw"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"orig_nbformat":4,"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}